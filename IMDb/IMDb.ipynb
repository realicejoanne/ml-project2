{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.1"
    },
    "colab": {
      "name": "Lecture 19 - 2 Keras IMDB CNN.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kMXsWTzkYVm7",
        "colab_type": "text"
      },
      "source": [
        "# IMDB movie review sentiment classification with CNNs\n",
        "\n",
        "In this notebook, we'll train a convolutional neural network (CNN, ConvNet) for sentiment classification using Keras.  Keras version $\\ge$ 2 is required.  This notebook is largely based on the [`imdb_cnn.py` script](https://github.com/keras-team/keras/blob/master/examples/imdb_cnn.py) in the Keras examples.\n",
        "\n",
        "First, the needed imports. Keras tells us which backend (Theano, Tensorflow, CNTK) it will be using."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qB8NiQq4YVm9",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 81
        },
        "outputId": "206cb98c-2e68-4ef9-f8e7-f5bf0bf70cef"
      },
      "source": [
        "# Basic Libraries\n",
        "%matplotlib inline\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "sns.set()\n",
        "from sklearn.model_selection import train_test_split\n",
        "# Keras\n",
        "from keras.preprocessing import sequence\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Dropout, Activation\n",
        "from keras.layers import Embedding\n",
        "from keras.layers import Conv1D, MaxPooling1D, GlobalMaxPooling1D,GlobalAveragePooling1D\n",
        "from keras import backend as K\n",
        "# Keras\n",
        "from keras import backend as K\n",
        "from keras.callbacks import ModelCheckpoint, EarlyStopping\n",
        "from keras.layers import Dense, Dropout, Activation\n",
        "from keras.layers import Embedding, Flatten\n",
        "from keras.layers import Conv1D, MaxPooling1D,AveragePooling1D, GlobalMaxPooling1D,GlobalAveragePooling1D\n",
        "from keras.layers.core import Activation, Dropout, Dense\n",
        "from keras.layers.embeddings import Embedding\n",
        "from keras.models import Sequential, load_model\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.preprocessing.text import one_hot\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from keras.preprocessing import sequence\n",
        "from keras.utils.vis_utils import model_to_dot\n",
        "\n",
        "# Libraries for Display Dot Model\n",
        "from IPython.display import SVG\n",
        "from keras.utils.vis_utils import model_to_dot"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<p style=\"color: red;\">\n",
              "The default version of TensorFlow in Colab will soon switch to TensorFlow 2.x.<br>\n",
              "We recommend you <a href=\"https://www.tensorflow.org/guide/migrate\" target=\"_blank\">upgrade</a> now \n",
              "or ensure your notebook will continue to use TensorFlow 1.x via the <code>%tensorflow_version 1.x</code> magic:\n",
              "<a href=\"https://colab.research.google.com/notebooks/tensorflow_version.ipynb\" target=\"_blank\">more info</a>.</p>\n"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TO1fCVsLksK-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aHIUMVTPYVnB",
        "colab_type": "text"
      },
      "source": [
        "## IMDB data set\n",
        "\n",
        "Next we'll load the IMDB data set. First time we may have to download the data, which can take a while.\n",
        "\n",
        "The dataset contains 50000 movies reviews from the Internet Movie Database, split into 25000 reviews for training and 25000 reviews for testing. Half of the reviews are positive (1) and half are negative (0).\n",
        "\n",
        "The dataset has already been preprocessed, and each word has been replaced by an integer index.\n",
        "The reviews are thus represented as varying-length sequences of integers.\n",
        "(Word indices begin at \"3\", as \"1\" is used to mark the start of a review and \"2\" represents all out-of-vocabulary words. \"0\" will be used later to pad shorter reviews to a fixed size.)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6MEQE_oMYVnC",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "outputId": "6ae6047f-9f55-4b4b-d908-3009a3f3e7c2"
      },
      "source": [
        "from keras.datasets import imdb\n",
        "\n",
        "# jumlah kata terbanyak yang digunakan\n",
        "nb_words = 10000\n",
        "\n",
        "print('Loading data...')\n",
        "(x_train, y_train), (x_test, y_test) = imdb.load_data(num_words=nb_words)\n",
        "word_index = imdb.get_word_index()\n",
        "print('Complete')"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Loading data...\n",
            "Complete\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Eda7W7l0YVnF",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 105
        },
        "outputId": "7e69184e-4e58-46b5-b1fd-4933c476e6c2"
      },
      "source": [
        "print('IMDB data loaded:')\n",
        "print('x_train:', x_train.shape)\n",
        "print('y_train:', y_train.shape, 'positive:', np.sum(y_train))\n",
        "print('x_test:', x_test.shape)\n",
        "print('y_test:', y_test.shape, 'positive:', np.sum(y_test))"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "IMDB data loaded:\n",
            "x_train: (25000,)\n",
            "y_train: (25000,) positive: 12500\n",
            "x_test: (25000,)\n",
            "y_test: (25000,) positive: 12500\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-0Iumm_AYVnH",
        "colab_type": "text"
      },
      "source": [
        "The first movie review in the training set:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b1jw0Fm5YVnI",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 72
        },
        "outputId": "7279f298-d3cc-4ec3-843b-3ee7a8815e73"
      },
      "source": [
        "print(\"First review in the training set:\\n\", x_train[0], \"length:\", len(x_train[0]), \"class:\", y_train[0])"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "First review in the training set:\n",
            " [1, 14, 22, 16, 43, 530, 973, 1622, 1385, 65, 458, 4468, 66, 3941, 4, 173, 36, 256, 5, 25, 100, 43, 838, 112, 50, 670, 2, 9, 35, 480, 284, 5, 150, 4, 172, 112, 167, 2, 336, 385, 39, 4, 172, 4536, 1111, 17, 546, 38, 13, 447, 4, 192, 50, 16, 6, 147, 2025, 19, 14, 22, 4, 1920, 4613, 469, 4, 22, 71, 87, 12, 16, 43, 530, 38, 76, 15, 13, 1247, 4, 22, 17, 515, 17, 12, 16, 626, 18, 2, 5, 62, 386, 12, 8, 316, 8, 106, 5, 4, 2223, 5244, 16, 480, 66, 3785, 33, 4, 130, 12, 16, 38, 619, 5, 25, 124, 51, 36, 135, 48, 25, 1415, 33, 6, 22, 12, 215, 28, 77, 52, 5, 14, 407, 16, 82, 2, 8, 4, 107, 117, 5952, 15, 256, 4, 2, 7, 3766, 5, 723, 36, 71, 43, 530, 476, 26, 400, 317, 46, 7, 4, 2, 1029, 13, 104, 88, 4, 381, 15, 297, 98, 32, 2071, 56, 26, 141, 6, 194, 7486, 18, 4, 226, 22, 21, 134, 476, 26, 480, 5, 144, 30, 5535, 18, 51, 36, 28, 224, 92, 25, 104, 4, 226, 65, 16, 38, 1334, 88, 12, 16, 283, 5, 16, 4472, 113, 103, 32, 15, 16, 5345, 19, 178, 32] length: 218 class: 1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "esztPZgPYVnL",
        "colab_type": "text"
      },
      "source": [
        "As a sanity check, we can convert the review back to text:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V_KlknHgYVnM",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "outputId": "073b0d43-0dd7-4f45-b68d-83e08c0dbb86"
      },
      "source": [
        "reverse_word_index = dict([(value, key) for (key, value) in word_index.items()])\n",
        "decoded_review = ' '.join([reverse_word_index.get(i - 3, '?') for i in x_train[0]])\n",
        "print(decoded_review)"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "? this film was just brilliant casting location scenery story direction everyone's really suited the part they played and you could just imagine being there robert ? is an amazing actor and now the same being director ? father came from the same scottish island as myself so i loved the fact there was a real connection with this film the witty remarks throughout the film were great it was just brilliant so much that i bought the film as soon as it was released for ? and would recommend it to everyone to watch and the fly fishing was amazing really cried at the end it was so sad and you know what they say if you cry at a film it must have been good and this definitely was also ? to the two little boy's that played the ? of norman and paul they were just brilliant children are often left out of the ? list i think because the stars that play them all grown up are such a big profile for the whole film but these children are amazing and should be praised for what they have done don't you think the whole story was so lovely because it was true and was someone's life after all that was shared with us all\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Qq6_RzZeYVnO",
        "colab_type": "text"
      },
      "source": [
        "The training data consists of lists of word indices of varying length.  Let's inspect the distribution of the length of the training movie reviews: "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OaGrRagdYVnP",
        "colab_type": "text"
      },
      "source": [
        "### Prepare The Data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HO3JTNSNYVnQ",
        "colab_type": "text"
      },
      "source": [
        "The reviews—the arrays of integers—must be converted to tensors before fed into the neural network:\n",
        "\n",
        "We can pad the arrays so they all have the same length, then create an integer tensor of shape `max_length * num_reviews`. We can use an embedding layer capable of handling this shape as the first layer in our network."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "syoPViz8YVnR",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 284
        },
        "outputId": "baca5222-f54a-4d3b-f8f2-f8b132b1f1c3"
      },
      "source": [
        "l = []\n",
        "for i in range(len(x_train)):\n",
        "    l.append(len(x_train[i]))\n",
        "plt.figure()\n",
        "plt.title('Length of training reviews')\n",
        "plt.hist(l,100);"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYIAAAELCAYAAADURYGZAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nRK5CYII=\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gi8RIYrTYVnU",
        "colab_type": "text"
      },
      "source": [
        "Since the movie reviews must be the same length, we will use the `pad_sequences` function to standardize the lengths"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xcb-e0TfYVnV",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 689
        },
        "outputId": "38a747ae-2397-4707-9c04-908f73ec70dc"
      },
      "source": [
        "maxlen = 256\n",
        "\n",
        "print('Pad sequences (samples x time)')\n",
        "x_train = sequence.pad_sequences(x_train, maxlen=maxlen,\n",
        "                                 padding='post', truncating='post')\n",
        "x_test = sequence.pad_sequences(x_test, maxlen=maxlen, \n",
        "                                padding='post', truncating='post')\n",
        "print('x_train:', x_train.shape)\n",
        "print('x_test:', x_test.shape)\n",
        "\n",
        "print(\"First review in the training set:\\n\", x_train[0], 'length:', len(x_train[0]))\n",
        "\n",
        "l = []\n",
        "for i in range(len(x_train)):\n",
        "    l.append(len(x_train[i]))\n",
        "plt.figure()\n",
        "plt.title('Length of training reviews')\n",
        "plt.hist(l,100);"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Pad sequences (samples x time)\n",
            "x_train: (25000, 256)\n",
            "x_test: (25000, 256)\n",
            "First review in the training set:\n",
            " [   1   14   22   16   43  530  973 1622 1385   65  458 4468   66 3941\n",
            "    4  173   36  256    5   25  100   43  838  112   50  670    2    9\n",
            "   35  480  284    5  150    4  172  112  167    2  336  385   39    4\n",
            "  172 4536 1111   17  546   38   13  447    4  192   50   16    6  147\n",
            " 2025   19   14   22    4 1920 4613  469    4   22   71   87   12   16\n",
            "   43  530   38   76   15   13 1247    4   22   17  515   17   12   16\n",
            "  626   18    2    5   62  386   12    8  316    8  106    5    4 2223\n",
            " 5244   16  480   66 3785   33    4  130   12   16   38  619    5   25\n",
            "  124   51   36  135   48   25 1415   33    6   22   12  215   28   77\n",
            "   52    5   14  407   16   82    2    8    4  107  117 5952   15  256\n",
            "    4    2    7 3766    5  723   36   71   43  530  476   26  400  317\n",
            "   46    7    4    2 1029   13  104   88    4  381   15  297   98   32\n",
            " 2071   56   26  141    6  194 7486   18    4  226   22   21  134  476\n",
            "   26  480    5  144   30 5535   18   51   36   28  224   92   25  104\n",
            "    4  226   65   16   38 1334   88   12   16  283    5   16 4472  113\n",
            "  103   32   15   16 5345   19  178   32    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0] length: 256\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYkAAAELCAYAAAAspXpuAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iQA6Eyd8YVnX",
        "colab_type": "text"
      },
      "source": [
        "# 1D CNN Model - Convolutional Layers"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J62cCKzfYVnY",
        "colab_type": "text"
      },
      "source": [
        "## Initialization\n",
        "\n",
        "Let's create a 1D CNN model that has one convolutional layers with *relu* as the activation function, followed by a *Dense* layer.  The first layer in the network is an *Embedding* layer that converts integer indices to dense vectors of length `embedding_dims`.  Dropout is applied after embedding and dense layers, and max pooling after the convolutional layers. The output layer contains a single neuron and *sigmoid* non-linearity to match the binary groundtruth (`y_train`). \n",
        "\n",
        "Finally, we `compile()` the model, using *binary crossentropy* as the loss function and [*RMSprop*](https://keras.io/optimizers/#rmsprop) as the optimizer."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZZRcblr6YVnZ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 776
        },
        "outputId": "8b89c6b1-4fcb-41dd-c3eb-1147aefe5afd"
      },
      "source": [
        "# model parameter:\n",
        "nb_words = 10000\n",
        "maxlen = 256\n",
        "\n",
        "embedding_dims = 50\n",
        "cnn_filters = 100\n",
        "cnn_kernel_size = 5\n",
        "dense_hidden_dims = 200\n",
        "\n",
        "print('Build model...')\n",
        "model = Sequential()\n",
        "\n",
        "model.add(Embedding(nb_words, embedding_dims, input_length=maxlen)) \n",
        "model.add(Dropout(0.2))\n",
        "\n",
        "#conv 1\n",
        "\n",
        "model.add(Conv1D(cnn_filters, cnn_kernel_size, padding='valid', activation='sigmoid'))\n",
        "model.add(GlobalAveragePooling1D())\n",
        "\n",
        "model.add(Dense(dense_hidden_dims, activation='relu'))\n",
        "model.add(Dropout(0.2))\n",
        "\n",
        "model.add(Dense(1, activation='sigmoid'))\n",
        "\n",
        "model.compile(loss='binary_crossentropy',\n",
        "              optimizer='rmsprop',\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "print(model.summary())"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Build model...\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:66: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:541: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:4432: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:148: The name tf.placeholder_with_default is deprecated. Please use tf.compat.v1.placeholder_with_default instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3733: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/optimizers.py:793: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3657: The name tf.log is deprecated. Please use tf.math.log instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/ops/nn_impl.py:183: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
            "Model: \"sequential_1\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding_1 (Embedding)      (None, 256, 50)           500000    \n",
            "_________________________________________________________________\n",
            "dropout_1 (Dropout)          (None, 256, 50)           0         \n",
            "_________________________________________________________________\n",
            "conv1d_1 (Conv1D)            (None, 252, 100)          25100     \n",
            "_________________________________________________________________\n",
            "global_average_pooling1d_1 ( (None, 100)               0         \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (None, 200)               20200     \n",
            "_________________________________________________________________\n",
            "dropout_2 (Dropout)          (None, 200)               0         \n",
            "_________________________________________________________________\n",
            "dense_2 (Dense)              (None, 1)                 201       \n",
            "=================================================================\n",
            "Total params: 545,501\n",
            "Trainable params: 545,501\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r3FUfGYPYVnd",
        "colab_type": "text"
      },
      "source": [
        "The layers are stacked sequentially to build the classifier:\n",
        "\n",
        "- The first layer is an Embedding layer. This layer takes the integer-encoded vocabulary and looks up the embedding vector for each word-index. These vectors are learned as the model trains. The vectors add a dimension to the output array. The resulting dimensions are: (batch, sequence, embedding).\n",
        "- Next, a GlobalMaxPooling1D layer returns a fixed-length output vector for each example by searching max value over the sequence dimension. This allows the model to handle input of variable length, in the simplest way possible.\n",
        "- This fixed-length output vector is piped through a fully-connected (Dense) layer.\n",
        "- The last layer is densely connected with a single output node. Using the sigmoid activation function, this value is a float between 0 and 1, representing a probability, or confidence level.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nxJTVsU5YVne",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 610
        },
        "outputId": "fbbe9c11-918b-4883-b79c-c88d227c2847"
      },
      "source": [
        "SVG(model_to_dot(model, show_shapes=True,dpi=50).create(prog='dot', format='svg'))"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<IPython.core.display.SVG object>"
            ],
            "image/svg+xml": "<svg height=\"442pt\" viewBox=\"0.00 0.00 527.00 636.00\" width=\"366pt\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\n<g class=\"graph\" id=\"graph0\" transform=\"scale(.6944 .6944) rotate(0) translate(4 632)\">\n<title>G</title>\n<polygon fill=\"#ffffff\" points=\"-4,4 -4,-632 523,-632 523,4 -4,4\" stroke=\"transparent\"/>\n<!-- 139634987662752 -->\n<g class=\"node\" id=\"node1\">\n<title>139634987662752</title>\n<polygon fill=\"none\" points=\"84.5,-581.5 84.5,-627.5 434.5,-627.5 434.5,-581.5 84.5,-581.5\" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"187\" y=\"-600.8\">embedding_1_input: InputLayer</text>\n<polyline fill=\"none\" points=\"289.5,-581.5 289.5,-627.5 \" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"318.5\" y=\"-612.3\">input:</text>\n<polyline fill=\"none\" points=\"289.5,-604.5 347.5,-604.5 \" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"318.5\" y=\"-589.3\">output:</text>\n<polyline fill=\"none\" points=\"347.5,-581.5 347.5,-627.5 \" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"391\" y=\"-612.3\">(None, 256)</text>\n<polyline fill=\"none\" points=\"347.5,-604.5 434.5,-604.5 \" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"391\" y=\"-589.3\">(None, 256)</text>\n</g>\n<!-- 139634987661184 -->\n<g class=\"node\" id=\"node2\">\n<title>139634987661184</title>\n<polygon fill=\"none\" points=\"90,-498.5 90,-544.5 429,-544.5 429,-498.5 90,-498.5\" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"175.5\" y=\"-517.8\">embedding_1: Embedding</text>\n<polyline fill=\"none\" points=\"261,-498.5 261,-544.5 \" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"290\" y=\"-529.3\">input:</text>\n<polyline fill=\"none\" points=\"261,-521.5 319,-521.5 \" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"290\" y=\"-506.3\">output:</text>\n<polyline fill=\"none\" points=\"319,-498.5 319,-544.5 \" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"374\" y=\"-529.3\">(None, 256)</text>\n<polyline fill=\"none\" points=\"319,-521.5 429,-521.5 \" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"374\" y=\"-506.3\">(None, 256, 50)</text>\n</g>\n<!-- 139634987662752&#45;&gt;139634987661184 -->\n<g class=\"edge\" id=\"edge1\">\n<title>139634987662752-&gt;139634987661184</title>\n<path d=\"M259.5,-581.3799C259.5,-573.1745 259.5,-563.7679 259.5,-554.8786\" fill=\"none\" stroke=\"#000000\"/>\n<polygon fill=\"#000000\" points=\"263.0001,-554.784 259.5,-544.784 256.0001,-554.784 263.0001,-554.784\" stroke=\"#000000\"/>\n</g>\n<!-- 139634987659616 -->\n<g class=\"node\" id=\"node3\">\n<title>139634987659616</title>\n<polygon fill=\"none\" points=\"108.5,-415.5 108.5,-461.5 410.5,-461.5 410.5,-415.5 108.5,-415.5\" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"175.5\" y=\"-434.8\">dropout_1: Dropout</text>\n<polyline fill=\"none\" points=\"242.5,-415.5 242.5,-461.5 \" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"271.5\" y=\"-446.3\">input:</text>\n<polyline fill=\"none\" points=\"242.5,-438.5 300.5,-438.5 \" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"271.5\" y=\"-423.3\">output:</text>\n<polyline fill=\"none\" points=\"300.5,-415.5 300.5,-461.5 \" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"355.5\" y=\"-446.3\">(None, 256, 50)</text>\n<polyline fill=\"none\" points=\"300.5,-438.5 410.5,-438.5 \" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"355.5\" y=\"-423.3\">(None, 256, 50)</text>\n</g>\n<!-- 139634987661184&#45;&gt;139634987659616 -->\n<g class=\"edge\" id=\"edge2\">\n<title>139634987661184-&gt;139634987659616</title>\n<path d=\"M259.5,-498.3799C259.5,-490.1745 259.5,-480.7679 259.5,-471.8786\" fill=\"none\" stroke=\"#000000\"/>\n<polygon fill=\"#000000\" points=\"263.0001,-471.784 259.5,-461.784 256.0001,-471.784 263.0001,-471.784\" stroke=\"#000000\"/>\n</g>\n<!-- 139634977507088 -->\n<g class=\"node\" id=\"node4\">\n<title>139634977507088</title>\n<polygon fill=\"none\" points=\"105.5,-332.5 105.5,-378.5 413.5,-378.5 413.5,-332.5 105.5,-332.5\" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"172\" y=\"-351.8\">conv1d_1: Conv1D</text>\n<polyline fill=\"none\" points=\"238.5,-332.5 238.5,-378.5 \" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"267.5\" y=\"-363.3\">input:</text>\n<polyline fill=\"none\" points=\"238.5,-355.5 296.5,-355.5 \" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"267.5\" y=\"-340.3\">output:</text>\n<polyline fill=\"none\" points=\"296.5,-332.5 296.5,-378.5 \" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"355\" y=\"-363.3\">(None, 256, 50)</text>\n<polyline fill=\"none\" points=\"296.5,-355.5 413.5,-355.5 \" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"355\" y=\"-340.3\">(None, 252, 100)</text>\n</g>\n<!-- 139634987659616&#45;&gt;139634977507088 -->\n<g class=\"edge\" id=\"edge3\">\n<title>139634987659616-&gt;139634977507088</title>\n<path d=\"M259.5,-415.3799C259.5,-407.1745 259.5,-397.7679 259.5,-388.8786\" fill=\"none\" stroke=\"#000000\"/>\n<polygon fill=\"#000000\" points=\"263.0001,-388.784 259.5,-378.784 256.0001,-388.784 263.0001,-388.784\" stroke=\"#000000\"/>\n</g>\n<!-- 139635043849720 -->\n<g class=\"node\" id=\"node5\">\n<title>139635043849720</title>\n<polygon fill=\"none\" points=\"0,-249.5 0,-295.5 519,-295.5 519,-249.5 0,-249.5\" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"172\" y=\"-268.8\">global_average_pooling1d_1: GlobalAveragePooling1D</text>\n<polyline fill=\"none\" points=\"344,-249.5 344,-295.5 \" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"373\" y=\"-280.3\">input:</text>\n<polyline fill=\"none\" points=\"344,-272.5 402,-272.5 \" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"373\" y=\"-257.3\">output:</text>\n<polyline fill=\"none\" points=\"402,-249.5 402,-295.5 \" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"460.5\" y=\"-280.3\">(None, 252, 100)</text>\n<polyline fill=\"none\" points=\"402,-272.5 519,-272.5 \" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"460.5\" y=\"-257.3\">(None, 100)</text>\n</g>\n<!-- 139634977507088&#45;&gt;139635043849720 -->\n<g class=\"edge\" id=\"edge4\">\n<title>139634977507088-&gt;139635043849720</title>\n<path d=\"M259.5,-332.3799C259.5,-324.1745 259.5,-314.7679 259.5,-305.8786\" fill=\"none\" stroke=\"#000000\"/>\n<polygon fill=\"#000000\" points=\"263.0001,-305.784 259.5,-295.784 256.0001,-305.784 263.0001,-305.784\" stroke=\"#000000\"/>\n</g>\n<!-- 139635208580120 -->\n<g class=\"node\" id=\"node6\">\n<title>139635208580120</title>\n<polygon fill=\"none\" points=\"133.5,-166.5 133.5,-212.5 385.5,-212.5 385.5,-166.5 133.5,-166.5\" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"187\" y=\"-185.8\">dense_1: Dense</text>\n<polyline fill=\"none\" points=\"240.5,-166.5 240.5,-212.5 \" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"269.5\" y=\"-197.3\">input:</text>\n<polyline fill=\"none\" points=\"240.5,-189.5 298.5,-189.5 \" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"269.5\" y=\"-174.3\">output:</text>\n<polyline fill=\"none\" points=\"298.5,-166.5 298.5,-212.5 \" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"342\" y=\"-197.3\">(None, 100)</text>\n<polyline fill=\"none\" points=\"298.5,-189.5 385.5,-189.5 \" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"342\" y=\"-174.3\">(None, 200)</text>\n</g>\n<!-- 139635043849720&#45;&gt;139635208580120 -->\n<g class=\"edge\" id=\"edge5\">\n<title>139635043849720-&gt;139635208580120</title>\n<path d=\"M259.5,-249.3799C259.5,-241.1745 259.5,-231.7679 259.5,-222.8786\" fill=\"none\" stroke=\"#000000\"/>\n<polygon fill=\"#000000\" points=\"263.0001,-222.784 259.5,-212.784 256.0001,-222.784 263.0001,-222.784\" stroke=\"#000000\"/>\n</g>\n<!-- 139634923159392 -->\n<g class=\"node\" id=\"node7\">\n<title>139634923159392</title>\n<polygon fill=\"none\" points=\"120,-83.5 120,-129.5 399,-129.5 399,-83.5 120,-83.5\" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"187\" y=\"-102.8\">dropout_2: Dropout</text>\n<polyline fill=\"none\" points=\"254,-83.5 254,-129.5 \" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"283\" y=\"-114.3\">input:</text>\n<polyline fill=\"none\" points=\"254,-106.5 312,-106.5 \" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"283\" y=\"-91.3\">output:</text>\n<polyline fill=\"none\" points=\"312,-83.5 312,-129.5 \" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"355.5\" y=\"-114.3\">(None, 200)</text>\n<polyline fill=\"none\" points=\"312,-106.5 399,-106.5 \" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"355.5\" y=\"-91.3\">(None, 200)</text>\n</g>\n<!-- 139635208580120&#45;&gt;139634923159392 -->\n<g class=\"edge\" id=\"edge6\">\n<title>139635208580120-&gt;139634923159392</title>\n<path d=\"M259.5,-166.3799C259.5,-158.1745 259.5,-148.7679 259.5,-139.8786\" fill=\"none\" stroke=\"#000000\"/>\n<polygon fill=\"#000000\" points=\"263.0001,-139.784 259.5,-129.784 256.0001,-139.784 263.0001,-139.784\" stroke=\"#000000\"/>\n</g>\n<!-- 139634928350432 -->\n<g class=\"node\" id=\"node8\">\n<title>139634928350432</title>\n<polygon fill=\"none\" points=\"133.5,-.5 133.5,-46.5 385.5,-46.5 385.5,-.5 133.5,-.5\" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"187\" y=\"-19.8\">dense_2: Dense</text>\n<polyline fill=\"none\" points=\"240.5,-.5 240.5,-46.5 \" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"269.5\" y=\"-31.3\">input:</text>\n<polyline fill=\"none\" points=\"240.5,-23.5 298.5,-23.5 \" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"269.5\" y=\"-8.3\">output:</text>\n<polyline fill=\"none\" points=\"298.5,-.5 298.5,-46.5 \" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"342\" y=\"-31.3\">(None, 200)</text>\n<polyline fill=\"none\" points=\"298.5,-23.5 385.5,-23.5 \" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"342\" y=\"-8.3\">(None, 1)</text>\n</g>\n<!-- 139634923159392&#45;&gt;139634928350432 -->\n<g class=\"edge\" id=\"edge7\">\n<title>139634923159392-&gt;139634928350432</title>\n<path d=\"M259.5,-83.3799C259.5,-75.1745 259.5,-65.7679 259.5,-56.8786\" fill=\"none\" stroke=\"#000000\"/>\n<polygon fill=\"#000000\" points=\"263.0001,-56.784 259.5,-46.784 256.0001,-56.784 263.0001,-56.784\" stroke=\"#000000\"/>\n</g>\n</g>\n</svg>"
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zUs-i-p_YVng",
        "colab_type": "text"
      },
      "source": [
        "## Learning\n",
        "\n",
        "Now we are ready to train our model.  An *epoch* means one pass through the whole training data. Note also that we are using a fraction of the training data as our validation set."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7rNIrM6OYVnh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "pat = 5 #ini adalah jumlah epoch tanpa peningkatan setelah pelatihan akan berhenti\n",
        "early_stopping = EarlyStopping(monitor='val_loss', patience=pat, verbose=1)\n",
        "\n",
        "#tentukan callback model pos pemeriksaan -> ini akan terus menyimpan model sebagai file fisik\n",
        "model_checkpoint = ModelCheckpoint('fas_mnist_1.h5', verbose=1, save_best_only=True)\n",
        "\n",
        "def fit_and_evaluate(t_x, val_x, t_y, val_y, EPOCHS=20, BATCH_SIZE=128):\n",
        "    results = model.fit(t_x, t_y, epochs=EPOCHS, batch_size=BATCH_SIZE, callbacks=[early_stopping, model_checkpoint], \n",
        "              verbose=1, validation_split=0.1)\n",
        "    print(\"Val Score: \", model.evaluate(val_x, val_y))\n",
        "    return results"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r-b2LMeHkc77",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "d2b97825-225e-42b7-a355-0f14141f9d30"
      },
      "source": [
        "n_folds=10\n",
        "epochs=5\n",
        "batch_size=128\n",
        "\n",
        "#simpan histori model dalam daftar setelah pemasangan sehingga kita dapat merencanakannya nanti\n",
        "model_history = [] \n",
        "\n",
        "for i in range(n_folds):\n",
        "    print(\"Training on Fold: \",i+1)\n",
        "    # train = tokenizeX(reviews_train_clean)\n",
        "    t_x, val_x, t_y, val_y = train_test_split(x_train, y_train, test_size=0.1, \n",
        "                                               random_state = np.random.randint(1,1000, 1)[0])\n",
        "    model_history.append(fit_and_evaluate(t_x, val_x, t_y, val_y, epochs, batch_size))\n",
        "    print(\"=======\"*12, end=\"\\n\\n\\n\")"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Training on Fold:  1\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:1033: The name tf.assign_add is deprecated. Please use tf.compat.v1.assign_add instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:1020: The name tf.assign is deprecated. Please use tf.compat.v1.assign instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3005: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.\n",
            "\n",
            "Train on 20250 samples, validate on 2250 samples\n",
            "Epoch 1/5\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:190: The name tf.get_default_session is deprecated. Please use tf.compat.v1.get_default_session instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:197: The name tf.ConfigProto is deprecated. Please use tf.compat.v1.ConfigProto instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:207: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:216: The name tf.is_variable_initialized is deprecated. Please use tf.compat.v1.is_variable_initialized instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:223: The name tf.variables_initializer is deprecated. Please use tf.compat.v1.variables_initializer instead.\n",
            "\n",
            "20250/20250 [==============================] - 3s 159us/step - loss: 0.6491 - acc: 0.6088 - val_loss: 0.4987 - val_acc: 0.8080\n",
            "\n",
            "Epoch 00001: val_loss improved from inf to 0.49874, saving model to fas_mnist_1.h5\n",
            "Epoch 2/5\n",
            "20250/20250 [==============================] - 1s 43us/step - loss: 0.3890 - acc: 0.8352 - val_loss: 0.5181 - val_acc: 0.7453\n",
            "\n",
            "Epoch 00002: val_loss did not improve from 0.49874\n",
            "Epoch 3/5\n",
            "20250/20250 [==============================] - 1s 45us/step - loss: 0.2976 - acc: 0.8766 - val_loss: 0.3013 - val_acc: 0.8813\n",
            "\n",
            "Epoch 00003: val_loss improved from 0.49874 to 0.30128, saving model to fas_mnist_1.h5\n",
            "Epoch 4/5\n",
            "20250/20250 [==============================] - 1s 44us/step - loss: 0.2558 - acc: 0.8970 - val_loss: 0.2992 - val_acc: 0.8782\n",
            "\n",
            "Epoch 00004: val_loss improved from 0.30128 to 0.29922, saving model to fas_mnist_1.h5\n",
            "Epoch 5/5\n",
            "20250/20250 [==============================] - 1s 43us/step - loss: 0.2293 - acc: 0.9102 - val_loss: 0.3117 - val_acc: 0.8711\n",
            "\n",
            "Epoch 00005: val_loss did not improve from 0.29922\n",
            "2500/2500 [==============================] - 0s 65us/step\n",
            "Val Score:  [0.32043151860237123, 0.878]\n",
            "====================================================================================\n",
            "\n",
            "\n",
            "Training on Fold:  2\n",
            "Train on 20250 samples, validate on 2250 samples\n",
            "Epoch 1/5\n",
            "20250/20250 [==============================] - 1s 44us/step - loss: 0.2320 - acc: 0.9114 - val_loss: 0.2165 - val_acc: 0.9182\n",
            "\n",
            "Epoch 00001: val_loss improved from 0.29922 to 0.21646, saving model to fas_mnist_1.h5\n",
            "Epoch 2/5\n",
            "20250/20250 [==============================] - 1s 45us/step - loss: 0.2135 - acc: 0.9174 - val_loss: 0.2253 - val_acc: 0.9133\n",
            "\n",
            "Epoch 00002: val_loss did not improve from 0.21646\n",
            "Epoch 3/5\n",
            "20250/20250 [==============================] - 1s 43us/step - loss: 0.1979 - acc: 0.9251 - val_loss: 0.2612 - val_acc: 0.8996\n",
            "\n",
            "Epoch 00003: val_loss did not improve from 0.21646\n",
            "Epoch 4/5\n",
            "20250/20250 [==============================] - 1s 44us/step - loss: 0.1842 - acc: 0.9300 - val_loss: 0.2456 - val_acc: 0.9071\n",
            "\n",
            "Epoch 00004: val_loss did not improve from 0.21646\n",
            "Epoch 5/5\n",
            "20250/20250 [==============================] - 1s 45us/step - loss: 0.1732 - acc: 0.9352 - val_loss: 0.2755 - val_acc: 0.8907\n",
            "\n",
            "Epoch 00005: val_loss did not improve from 0.21646\n",
            "2500/2500 [==============================] - 0s 41us/step\n",
            "Val Score:  [0.2544785229921341, 0.9036]\n",
            "====================================================================================\n",
            "\n",
            "\n",
            "Training on Fold:  3\n",
            "Train on 20250 samples, validate on 2250 samples\n",
            "Epoch 1/5\n",
            "20250/20250 [==============================] - 1s 46us/step - loss: 0.1885 - acc: 0.9293 - val_loss: 0.1783 - val_acc: 0.9324\n",
            "\n",
            "Epoch 00001: val_loss improved from 0.21646 to 0.17834, saving model to fas_mnist_1.h5\n",
            "Epoch 2/5\n",
            "20250/20250 [==============================] - 1s 44us/step - loss: 0.1766 - acc: 0.9347 - val_loss: 0.1906 - val_acc: 0.9262\n",
            "\n",
            "Epoch 00002: val_loss did not improve from 0.17834\n",
            "Epoch 3/5\n",
            "20250/20250 [==============================] - 1s 44us/step - loss: 0.1691 - acc: 0.9378 - val_loss: 0.2714 - val_acc: 0.8911\n",
            "\n",
            "Epoch 00003: val_loss did not improve from 0.17834\n",
            "Epoch 4/5\n",
            "20250/20250 [==============================] - 1s 45us/step - loss: 0.1572 - acc: 0.9419 - val_loss: 0.2477 - val_acc: 0.9044\n",
            "\n",
            "Epoch 00004: val_loss did not improve from 0.17834\n",
            "Epoch 5/5\n",
            "20250/20250 [==============================] - 1s 44us/step - loss: 0.1504 - acc: 0.9457 - val_loss: 0.2124 - val_acc: 0.9213\n",
            "\n",
            "Epoch 00005: val_loss did not improve from 0.17834\n",
            "2500/2500 [==============================] - 0s 37us/step\n",
            "Val Score:  [0.18738943158388138, 0.9312]\n",
            "====================================================================================\n",
            "\n",
            "\n",
            "Training on Fold:  4\n",
            "Train on 20250 samples, validate on 2250 samples\n",
            "Epoch 1/5\n",
            "20250/20250 [==============================] - 1s 44us/step - loss: 0.1604 - acc: 0.9433 - val_loss: 0.1407 - val_acc: 0.9533\n",
            "\n",
            "Epoch 00001: val_loss improved from 0.17834 to 0.14072, saving model to fas_mnist_1.h5\n",
            "Epoch 2/5\n",
            "20250/20250 [==============================] - 1s 44us/step - loss: 0.1509 - acc: 0.9469 - val_loss: 0.1594 - val_acc: 0.9440\n",
            "\n",
            "Epoch 00002: val_loss did not improve from 0.14072\n",
            "Epoch 3/5\n",
            "20250/20250 [==============================] - 1s 45us/step - loss: 0.1447 - acc: 0.9479 - val_loss: 0.1641 - val_acc: 0.9413\n",
            "\n",
            "Epoch 00003: val_loss did not improve from 0.14072\n",
            "Epoch 4/5\n",
            "20250/20250 [==============================] - 1s 44us/step - loss: 0.1393 - acc: 0.9505 - val_loss: 0.2577 - val_acc: 0.8924\n",
            "\n",
            "Epoch 00004: val_loss did not improve from 0.14072\n",
            "Epoch 5/5\n",
            "20250/20250 [==============================] - 1s 44us/step - loss: 0.1340 - acc: 0.9536 - val_loss: 0.3219 - val_acc: 0.8702\n",
            "\n",
            "Epoch 00005: val_loss did not improve from 0.14072\n",
            "2500/2500 [==============================] - 0s 40us/step\n",
            "Val Score:  [0.3271319936037064, 0.8816]\n",
            "====================================================================================\n",
            "\n",
            "\n",
            "Training on Fold:  5\n",
            "Train on 20250 samples, validate on 2250 samples\n",
            "Epoch 1/5\n",
            "20250/20250 [==============================] - 1s 45us/step - loss: 0.1499 - acc: 0.9459 - val_loss: 0.2884 - val_acc: 0.8698\n",
            "\n",
            "Epoch 00001: val_loss did not improve from 0.14072\n",
            "Epoch 2/5\n",
            "20250/20250 [==============================] - 1s 44us/step - loss: 0.1431 - acc: 0.9489 - val_loss: 0.1608 - val_acc: 0.9400\n",
            "\n",
            "Epoch 00002: val_loss did not improve from 0.14072\n",
            "Epoch 3/5\n",
            "20250/20250 [==============================] - 1s 43us/step - loss: 0.1333 - acc: 0.9533 - val_loss: 0.1739 - val_acc: 0.9320\n",
            "\n",
            "Epoch 00003: val_loss did not improve from 0.14072\n",
            "Epoch 4/5\n",
            "20250/20250 [==============================] - 1s 42us/step - loss: 0.1263 - acc: 0.9550 - val_loss: 0.1992 - val_acc: 0.9244\n",
            "\n",
            "Epoch 00004: val_loss did not improve from 0.14072\n",
            "Epoch 5/5\n",
            "20250/20250 [==============================] - 1s 44us/step - loss: 0.1240 - acc: 0.9555 - val_loss: 0.1769 - val_acc: 0.9364\n",
            "\n",
            "Epoch 00005: val_loss did not improve from 0.14072\n",
            "2500/2500 [==============================] - 0s 38us/step\n",
            "Val Score:  [0.19225502601861955, 0.934]\n",
            "====================================================================================\n",
            "\n",
            "\n",
            "Training on Fold:  6\n",
            "Train on 20250 samples, validate on 2250 samples\n",
            "Epoch 1/5\n",
            "20250/20250 [==============================] - 1s 43us/step - loss: 0.1335 - acc: 0.9529 - val_loss: 0.1264 - val_acc: 0.9573\n",
            "\n",
            "Epoch 00001: val_loss improved from 0.14072 to 0.12639, saving model to fas_mnist_1.h5\n",
            "Epoch 2/5\n",
            "20250/20250 [==============================] - 1s 46us/step - loss: 0.1291 - acc: 0.9544 - val_loss: 0.1613 - val_acc: 0.9364\n",
            "\n",
            "Epoch 00002: val_loss did not improve from 0.12639\n",
            "Epoch 3/5\n",
            "20250/20250 [==============================] - 1s 45us/step - loss: 0.1212 - acc: 0.9580 - val_loss: 0.1851 - val_acc: 0.9360\n",
            "\n",
            "Epoch 00003: val_loss did not improve from 0.12639\n",
            "Epoch 4/5\n",
            "20250/20250 [==============================] - 1s 43us/step - loss: 0.1190 - acc: 0.9582 - val_loss: 0.1947 - val_acc: 0.9316\n",
            "\n",
            "Epoch 00004: val_loss did not improve from 0.12639\n",
            "Epoch 5/5\n",
            "20250/20250 [==============================] - 1s 43us/step - loss: 0.1147 - acc: 0.9616 - val_loss: 0.1852 - val_acc: 0.9382\n",
            "\n",
            "Epoch 00005: val_loss did not improve from 0.12639\n",
            "2500/2500 [==============================] - 0s 46us/step\n",
            "Val Score:  [0.16408413544781505, 0.9388]\n",
            "====================================================================================\n",
            "\n",
            "\n",
            "Training on Fold:  7\n",
            "Train on 20250 samples, validate on 2250 samples\n",
            "Epoch 1/5\n",
            "20250/20250 [==============================] - 1s 45us/step - loss: 0.1270 - acc: 0.9547 - val_loss: 0.5242 - val_acc: 0.8004\n",
            "\n",
            "Epoch 00001: val_loss did not improve from 0.12639\n",
            "Epoch 2/5\n",
            "20250/20250 [==============================] - 1s 43us/step - loss: 0.1225 - acc: 0.9578 - val_loss: 0.1392 - val_acc: 0.9471\n",
            "\n",
            "Epoch 00002: val_loss did not improve from 0.12639\n",
            "Epoch 3/5\n",
            "20250/20250 [==============================] - 1s 43us/step - loss: 0.1171 - acc: 0.9586 - val_loss: 0.1736 - val_acc: 0.9329\n",
            "\n",
            "Epoch 00003: val_loss did not improve from 0.12639\n",
            "Epoch 4/5\n",
            "20250/20250 [==============================] - 1s 44us/step - loss: 0.1119 - acc: 0.9618 - val_loss: 0.1467 - val_acc: 0.9467\n",
            "\n",
            "Epoch 00004: val_loss did not improve from 0.12639\n",
            "Epoch 5/5\n",
            "20250/20250 [==============================] - 1s 45us/step - loss: 0.1076 - acc: 0.9624 - val_loss: 0.1536 - val_acc: 0.9449\n",
            "\n",
            "Epoch 00005: val_loss did not improve from 0.12639\n",
            "2500/2500 [==============================] - 0s 39us/step\n",
            "Val Score:  [0.14960566410236062, 0.944]\n",
            "====================================================================================\n",
            "\n",
            "\n",
            "Training on Fold:  8\n",
            "Train on 20250 samples, validate on 2250 samples\n",
            "Epoch 1/5\n",
            "20250/20250 [==============================] - 1s 44us/step - loss: 0.1167 - acc: 0.9601 - val_loss: 0.1008 - val_acc: 0.9680\n",
            "\n",
            "Epoch 00001: val_loss improved from 0.12639 to 0.10076, saving model to fas_mnist_1.h5\n",
            "Epoch 2/5\n",
            "20250/20250 [==============================] - 1s 44us/step - loss: 0.1165 - acc: 0.9589 - val_loss: 0.1147 - val_acc: 0.9582\n",
            "\n",
            "Epoch 00002: val_loss did not improve from 0.10076\n",
            "Epoch 3/5\n",
            "20250/20250 [==============================] - 1s 44us/step - loss: 0.1107 - acc: 0.9624 - val_loss: 0.1649 - val_acc: 0.9373\n",
            "\n",
            "Epoch 00003: val_loss did not improve from 0.10076\n",
            "Epoch 4/5\n",
            "20250/20250 [==============================] - 1s 44us/step - loss: 0.1023 - acc: 0.9655 - val_loss: 0.1787 - val_acc: 0.9298\n",
            "\n",
            "Epoch 00004: val_loss did not improve from 0.10076\n",
            "Epoch 5/5\n",
            "20250/20250 [==============================] - 1s 44us/step - loss: 0.1007 - acc: 0.9663 - val_loss: 0.2614 - val_acc: 0.9004\n",
            "\n",
            "Epoch 00005: val_loss did not improve from 0.10076\n",
            "2500/2500 [==============================] - 0s 44us/step\n",
            "Val Score:  [0.2544891058225185, 0.904]\n",
            "====================================================================================\n",
            "\n",
            "\n",
            "Training on Fold:  9\n",
            "Train on 20250 samples, validate on 2250 samples\n",
            "Epoch 1/5\n",
            "20250/20250 [==============================] - 1s 44us/step - loss: 0.1085 - acc: 0.9625 - val_loss: 0.1354 - val_acc: 0.9516\n",
            "\n",
            "Epoch 00001: val_loss did not improve from 0.10076\n",
            "Epoch 2/5\n",
            "20250/20250 [==============================] - 1s 44us/step - loss: 0.1019 - acc: 0.9646 - val_loss: 0.1782 - val_acc: 0.9338\n",
            "\n",
            "Epoch 00002: val_loss did not improve from 0.10076\n",
            "Epoch 3/5\n",
            "20250/20250 [==============================] - 1s 43us/step - loss: 0.0979 - acc: 0.9677 - val_loss: 0.1378 - val_acc: 0.9498\n",
            "\n",
            "Epoch 00003: val_loss did not improve from 0.10076\n",
            "Epoch 4/5\n",
            "20250/20250 [==============================] - 1s 43us/step - loss: 0.0975 - acc: 0.9678 - val_loss: 0.2432 - val_acc: 0.9138\n",
            "\n",
            "Epoch 00004: val_loss did not improve from 0.10076\n",
            "Epoch 5/5\n",
            "20250/20250 [==============================] - 1s 44us/step - loss: 0.0930 - acc: 0.9679 - val_loss: 0.2232 - val_acc: 0.9236\n",
            "\n",
            "Epoch 00005: val_loss did not improve from 0.10076\n",
            "2500/2500 [==============================] - 0s 39us/step\n",
            "Val Score:  [0.25394254995584487, 0.912]\n",
            "====================================================================================\n",
            "\n",
            "\n",
            "Training on Fold:  10\n",
            "Train on 20250 samples, validate on 2250 samples\n",
            "Epoch 1/5\n",
            "20250/20250 [==============================] - 1s 45us/step - loss: 0.1098 - acc: 0.9634 - val_loss: 0.0838 - val_acc: 0.9756\n",
            "\n",
            "Epoch 00001: val_loss improved from 0.10076 to 0.08378, saving model to fas_mnist_1.h5\n",
            "Epoch 2/5\n",
            "20250/20250 [==============================] - 1s 43us/step - loss: 0.1022 - acc: 0.9657 - val_loss: 0.1924 - val_acc: 0.9218\n",
            "\n",
            "Epoch 00002: val_loss did not improve from 0.08378\n",
            "Epoch 3/5\n",
            "20250/20250 [==============================] - 1s 44us/step - loss: 0.0984 - acc: 0.9676 - val_loss: 0.1992 - val_acc: 0.9182\n",
            "\n",
            "Epoch 00003: val_loss did not improve from 0.08378\n",
            "Epoch 4/5\n",
            "20250/20250 [==============================] - 1s 42us/step - loss: 0.0955 - acc: 0.9682 - val_loss: 0.1357 - val_acc: 0.9516\n",
            "\n",
            "Epoch 00004: val_loss did not improve from 0.08378\n",
            "Epoch 5/5\n",
            "20250/20250 [==============================] - 1s 43us/step - loss: 0.0929 - acc: 0.9683 - val_loss: 0.1346 - val_acc: 0.9529\n",
            "\n",
            "Epoch 00005: val_loss did not improve from 0.08378\n",
            "2500/2500 [==============================] - 0s 39us/step\n",
            "Val Score:  [0.15007865550518035, 0.9468]\n",
            "====================================================================================\n",
            "\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e4tFPsVOYVnk",
        "colab_type": "text"
      },
      "source": [
        "Let's plot the data to see how the training progressed. A big gap between training and validation accuracies would suggest overfitting."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rxMVJKGoYVnl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "29kvbLC6YVnn",
        "colab_type": "text"
      },
      "source": [
        "In this plot, the dots represent the training loss and accuracy, and the solid lines are the validation loss and accuracy.\n",
        "\n",
        "Notice the training loss decreases with each epoch and the training accuracy increases with each epoch. This is expected when using a gradient descent optimization—it should minimize the desired quantity on every iteration.\n",
        "\n",
        "This isn't the case for the validation loss and accuracy—they seem to peak after about eight epochs. This is an example of overfitting: the model performs better on the training data than it does on data it has never seen before. After this point, the model over-optimizes and learns representations specific to the training data that do not generalize to test data."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uNTnpgeNYVno",
        "colab_type": "text"
      },
      "source": [
        "## Inference\n",
        "\n",
        "For a better measure of the quality of the model, let's see the model accuracy for the test data. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DwPpE-z9YVno",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "294f1097-2d01-4398-ff9b-066575ebda99"
      },
      "source": [
        "scores = model.evaluate(x_test, y_test, verbose=2)\n",
        "print(\"%s: %.2f%%\" % (model.metrics_names[1], scores[1]*100))"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "acc: 83.29%\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M8Sgd0c_YVnr",
        "colab_type": "text"
      },
      "source": [
        "We can also use the learned model to predict sentiments for new reviews:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mrUzsjKtYVns",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 316
        },
        "outputId": "dbba1323-71c4-496b-dbe7-a030ac2ed880"
      },
      "source": [
        "myreviewtext = 'this movie was the worst i have ever seen and the actors were horrible'\n",
        "#myreviewtext = 'this movie is great and i madly love the plot from beginning to end'\n",
        "\n",
        "myreview = np.zeros((1,maxlen), dtype=int)\n",
        "myreview[0, 0] = 1\n",
        "\n",
        "for i, w in enumerate(myreviewtext.split()):\n",
        "    if w in word_index and word_index[w]+3<nb_words:\n",
        "        myreview[0, i+1] = word_index[w]+3\n",
        "    else:\n",
        "        print('word not in vocabulary:', w)\n",
        "        myreview[0, i+1] = 2\n",
        "\n",
        "print(myreview, \"shape:\", myreview.shape)\n",
        "\n",
        "p = model.predict(myreview, batch_size=1)\n",
        "print('Predicted sentiment: {:.10f}'.format(p[0,0]))\n",
        "print('Note: Values close to \"0\" mean negative, close to \"1\" positive')"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[  1  14  20  16   4 249  13  28 126 110   5   4 156  71 527   0   0   0\n",
            "    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
            "    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
            "    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
            "    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
            "    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
            "    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
            "    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
            "    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
            "    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
            "    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
            "    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
            "    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
            "    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
            "    0   0   0   0]] shape: (1, 256)\n",
            "Predicted sentiment: 0.1158103645\n",
            "Note: Values close to \"0\" mean negative, close to \"1\" positive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I_bWrBYArWld",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B_6Wi5uZYVnv",
        "colab_type": "text"
      },
      "source": [
        "# 1D CNN Model  - 2 convolutional layers\n",
        "\n",
        "## Initialization\n",
        "\n",
        "Let's create a 1D CNN model that has two convolutional layers with *relu* as the activation function, followed by a *Dense* layer.  The first layer in the network is an *Embedding* layer that converts integer indices to dense vectors of length `embedding_dims`.  Dropout is applied after embedding and dense layers, and max pooling after the convolutional layers. The output layer contains a single neuron and *sigmoid* non-linearity to match the binary groundtruth (`y_train`). \n",
        "\n",
        "Finally, we `compile()` the model, using *binary crossentropy* as the loss function and [*RMSprop*](https://keras.io/optimizers/#rmsprop) as the optimizer."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YIYwBQUQYVnw",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 615
        },
        "outputId": "f5203fc1-0832-4fbf-aa7f-75c77480777d"
      },
      "source": [
        "# model parameters:\n",
        "nb_words = 10000\n",
        "maxlen = 256\n",
        "\n",
        "embedding_dims = 50\n",
        "cnn_filters = 100\n",
        "cnn_kernel_size = 5\n",
        "dense_hidden_dims = 200\n",
        "\n",
        "print('Build model...')\n",
        "model = Sequential()\n",
        "\n",
        "model.add(Embedding(nb_words, embedding_dims, input_length=maxlen))\n",
        "model.add(Dropout(0.2))\n",
        "\n",
        "#conv 1\n",
        "\n",
        "model.add(Conv1D(cnn_filters, cnn_kernel_size, padding='valid', activation='sigmoid'))\n",
        "model.add(AveragePooling1D(5)) \n",
        "\n",
        "#conv 2\n",
        "\n",
        "model.add(Conv1D(cnn_filters, cnn_kernel_size, padding='valid', activation='sigmoid'))\n",
        "model.add(GlobalAveragePooling1D())\n",
        "\n",
        "model.add(Dense(dense_hidden_dims, activation='relu'))\n",
        "model.add(Dropout(0.2))\n",
        "\n",
        "model.add(Dense(dense_hidden_dims, activation='sigmoid'))\n",
        "model.add(Dropout(0.2))\n",
        "\n",
        "model.add(Dense(1, activation='sigmoid'))\n",
        "\n",
        "model.compile(loss='binary_crossentropy',\n",
        "              optimizer='rmsprop',\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "print(model.summary())"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Build model...\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:4271: The name tf.nn.avg_pool is deprecated. Please use tf.nn.avg_pool2d instead.\n",
            "\n",
            "Model: \"sequential_2\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding_2 (Embedding)      (None, 256, 50)           500000    \n",
            "_________________________________________________________________\n",
            "dropout_3 (Dropout)          (None, 256, 50)           0         \n",
            "_________________________________________________________________\n",
            "conv1d_2 (Conv1D)            (None, 252, 100)          25100     \n",
            "_________________________________________________________________\n",
            "average_pooling1d_1 (Average (None, 50, 100)           0         \n",
            "_________________________________________________________________\n",
            "conv1d_3 (Conv1D)            (None, 46, 100)           50100     \n",
            "_________________________________________________________________\n",
            "global_average_pooling1d_2 ( (None, 100)               0         \n",
            "_________________________________________________________________\n",
            "dense_3 (Dense)              (None, 200)               20200     \n",
            "_________________________________________________________________\n",
            "dropout_4 (Dropout)          (None, 200)               0         \n",
            "_________________________________________________________________\n",
            "dense_4 (Dense)              (None, 200)               40200     \n",
            "_________________________________________________________________\n",
            "dropout_5 (Dropout)          (None, 200)               0         \n",
            "_________________________________________________________________\n",
            "dense_5 (Dense)              (None, 1)                 201       \n",
            "=================================================================\n",
            "Total params: 635,801\n",
            "Trainable params: 635,801\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N5XP0VKjYVnz",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 916
        },
        "outputId": "db6af4fa-da90-4412-b41f-ff6fa8d26318"
      },
      "source": [
        "SVG(model_to_dot(model, show_shapes=True, dpi=50).create(prog='dot', format='svg'))"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<IPython.core.display.SVG object>"
            ],
            "image/svg+xml": "<svg height=\"672pt\" viewBox=\"0.00 0.00 520.00 968.00\" width=\"361pt\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\n<g class=\"graph\" id=\"graph0\" transform=\"scale(.6944 .6944) rotate(0) translate(4 964)\">\n<title>G</title>\n<polygon fill=\"#ffffff\" points=\"-4,4 -4,-964 516,-964 516,4 -4,4\" stroke=\"transparent\"/>\n<!-- 139634956321232 -->\n<g class=\"node\" id=\"node1\">\n<title>139634956321232</title>\n<polygon fill=\"none\" points=\"81,-913.5 81,-959.5 431,-959.5 431,-913.5 81,-913.5\" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"183.5\" y=\"-932.8\">embedding_2_input: InputLayer</text>\n<polyline fill=\"none\" points=\"286,-913.5 286,-959.5 \" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"315\" y=\"-944.3\">input:</text>\n<polyline fill=\"none\" points=\"286,-936.5 344,-936.5 \" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"315\" y=\"-921.3\">output:</text>\n<polyline fill=\"none\" points=\"344,-913.5 344,-959.5 \" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"387.5\" y=\"-944.3\">(None, 256)</text>\n<polyline fill=\"none\" points=\"344,-936.5 431,-936.5 \" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"387.5\" y=\"-921.3\">(None, 256)</text>\n</g>\n<!-- 139635211526272 -->\n<g class=\"node\" id=\"node2\">\n<title>139635211526272</title>\n<polygon fill=\"none\" points=\"86.5,-830.5 86.5,-876.5 425.5,-876.5 425.5,-830.5 86.5,-830.5\" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"172\" y=\"-849.8\">embedding_2: Embedding</text>\n<polyline fill=\"none\" points=\"257.5,-830.5 257.5,-876.5 \" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"286.5\" y=\"-861.3\">input:</text>\n<polyline fill=\"none\" points=\"257.5,-853.5 315.5,-853.5 \" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"286.5\" y=\"-838.3\">output:</text>\n<polyline fill=\"none\" points=\"315.5,-830.5 315.5,-876.5 \" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"370.5\" y=\"-861.3\">(None, 256)</text>\n<polyline fill=\"none\" points=\"315.5,-853.5 425.5,-853.5 \" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"370.5\" y=\"-838.3\">(None, 256, 50)</text>\n</g>\n<!-- 139634956321232&#45;&gt;139635211526272 -->\n<g class=\"edge\" id=\"edge1\">\n<title>139634956321232-&gt;139635211526272</title>\n<path d=\"M256,-913.3799C256,-905.1745 256,-895.7679 256,-886.8786\" fill=\"none\" stroke=\"#000000\"/>\n<polygon fill=\"#000000\" points=\"259.5001,-886.784 256,-876.784 252.5001,-886.784 259.5001,-886.784\" stroke=\"#000000\"/>\n</g>\n<!-- 139634956321064 -->\n<g class=\"node\" id=\"node3\">\n<title>139634956321064</title>\n<polygon fill=\"none\" points=\"105,-747.5 105,-793.5 407,-793.5 407,-747.5 105,-747.5\" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"172\" y=\"-766.8\">dropout_3: Dropout</text>\n<polyline fill=\"none\" points=\"239,-747.5 239,-793.5 \" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"268\" y=\"-778.3\">input:</text>\n<polyline fill=\"none\" points=\"239,-770.5 297,-770.5 \" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"268\" y=\"-755.3\">output:</text>\n<polyline fill=\"none\" points=\"297,-747.5 297,-793.5 \" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"352\" y=\"-778.3\">(None, 256, 50)</text>\n<polyline fill=\"none\" points=\"297,-770.5 407,-770.5 \" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"352\" y=\"-755.3\">(None, 256, 50)</text>\n</g>\n<!-- 139635211526272&#45;&gt;139634956321064 -->\n<g class=\"edge\" id=\"edge2\">\n<title>139635211526272-&gt;139634956321064</title>\n<path d=\"M256,-830.3799C256,-822.1745 256,-812.7679 256,-803.8786\" fill=\"none\" stroke=\"#000000\"/>\n<polygon fill=\"#000000\" points=\"259.5001,-803.784 256,-793.784 252.5001,-803.784 259.5001,-803.784\" stroke=\"#000000\"/>\n</g>\n<!-- 139634956320896 -->\n<g class=\"node\" id=\"node4\">\n<title>139634956320896</title>\n<polygon fill=\"none\" points=\"102,-664.5 102,-710.5 410,-710.5 410,-664.5 102,-664.5\" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"168.5\" y=\"-683.8\">conv1d_2: Conv1D</text>\n<polyline fill=\"none\" points=\"235,-664.5 235,-710.5 \" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"264\" y=\"-695.3\">input:</text>\n<polyline fill=\"none\" points=\"235,-687.5 293,-687.5 \" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"264\" y=\"-672.3\">output:</text>\n<polyline fill=\"none\" points=\"293,-664.5 293,-710.5 \" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"351.5\" y=\"-695.3\">(None, 256, 50)</text>\n<polyline fill=\"none\" points=\"293,-687.5 410,-687.5 \" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"351.5\" y=\"-672.3\">(None, 252, 100)</text>\n</g>\n<!-- 139634956321064&#45;&gt;139634956320896 -->\n<g class=\"edge\" id=\"edge3\">\n<title>139634956321064-&gt;139634956320896</title>\n<path d=\"M256,-747.3799C256,-739.1745 256,-729.7679 256,-720.8786\" fill=\"none\" stroke=\"#000000\"/>\n<polygon fill=\"#000000\" points=\"259.5001,-720.784 256,-710.784 252.5001,-720.784 259.5001,-720.784\" stroke=\"#000000\"/>\n</g>\n<!-- 139634956321288 -->\n<g class=\"node\" id=\"node5\">\n<title>139634956321288</title>\n<polygon fill=\"none\" points=\"38.5,-581.5 38.5,-627.5 473.5,-627.5 473.5,-581.5 38.5,-581.5\" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"168.5\" y=\"-600.8\">average_pooling1d_1: AveragePooling1D</text>\n<polyline fill=\"none\" points=\"298.5,-581.5 298.5,-627.5 \" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"327.5\" y=\"-612.3\">input:</text>\n<polyline fill=\"none\" points=\"298.5,-604.5 356.5,-604.5 \" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"327.5\" y=\"-589.3\">output:</text>\n<polyline fill=\"none\" points=\"356.5,-581.5 356.5,-627.5 \" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"415\" y=\"-612.3\">(None, 252, 100)</text>\n<polyline fill=\"none\" points=\"356.5,-604.5 473.5,-604.5 \" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"415\" y=\"-589.3\">(None, 50, 100)</text>\n</g>\n<!-- 139634956320896&#45;&gt;139634956321288 -->\n<g class=\"edge\" id=\"edge4\">\n<title>139634956320896-&gt;139634956321288</title>\n<path d=\"M256,-664.3799C256,-656.1745 256,-646.7679 256,-637.8786\" fill=\"none\" stroke=\"#000000\"/>\n<polygon fill=\"#000000\" points=\"259.5001,-637.784 256,-627.784 252.5001,-637.784 259.5001,-637.784\" stroke=\"#000000\"/>\n</g>\n<!-- 139634967866448 -->\n<g class=\"node\" id=\"node6\">\n<title>139634967866448</title>\n<polygon fill=\"none\" points=\"105.5,-498.5 105.5,-544.5 406.5,-544.5 406.5,-498.5 105.5,-498.5\" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"172\" y=\"-517.8\">conv1d_3: Conv1D</text>\n<polyline fill=\"none\" points=\"238.5,-498.5 238.5,-544.5 \" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"267.5\" y=\"-529.3\">input:</text>\n<polyline fill=\"none\" points=\"238.5,-521.5 296.5,-521.5 \" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"267.5\" y=\"-506.3\">output:</text>\n<polyline fill=\"none\" points=\"296.5,-498.5 296.5,-544.5 \" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"351.5\" y=\"-529.3\">(None, 50, 100)</text>\n<polyline fill=\"none\" points=\"296.5,-521.5 406.5,-521.5 \" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"351.5\" y=\"-506.3\">(None, 46, 100)</text>\n</g>\n<!-- 139634956321288&#45;&gt;139634967866448 -->\n<g class=\"edge\" id=\"edge5\">\n<title>139634956321288-&gt;139634967866448</title>\n<path d=\"M256,-581.3799C256,-573.1745 256,-563.7679 256,-554.8786\" fill=\"none\" stroke=\"#000000\"/>\n<polygon fill=\"#000000\" points=\"259.5001,-554.784 256,-544.784 252.5001,-554.784 259.5001,-554.784\" stroke=\"#000000\"/>\n</g>\n<!-- 139634967820608 -->\n<g class=\"node\" id=\"node7\">\n<title>139634967820608</title>\n<polygon fill=\"none\" points=\"0,-415.5 0,-461.5 512,-461.5 512,-415.5 0,-415.5\" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"172\" y=\"-434.8\">global_average_pooling1d_2: GlobalAveragePooling1D</text>\n<polyline fill=\"none\" points=\"344,-415.5 344,-461.5 \" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"373\" y=\"-446.3\">input:</text>\n<polyline fill=\"none\" points=\"344,-438.5 402,-438.5 \" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"373\" y=\"-423.3\">output:</text>\n<polyline fill=\"none\" points=\"402,-415.5 402,-461.5 \" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"457\" y=\"-446.3\">(None, 46, 100)</text>\n<polyline fill=\"none\" points=\"402,-438.5 512,-438.5 \" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"457\" y=\"-423.3\">(None, 100)</text>\n</g>\n<!-- 139634967866448&#45;&gt;139634967820608 -->\n<g class=\"edge\" id=\"edge6\">\n<title>139634967866448-&gt;139634967820608</title>\n<path d=\"M256,-498.3799C256,-490.1745 256,-480.7679 256,-471.8786\" fill=\"none\" stroke=\"#000000\"/>\n<polygon fill=\"#000000\" points=\"259.5001,-471.784 256,-461.784 252.5001,-471.784 259.5001,-471.784\" stroke=\"#000000\"/>\n</g>\n<!-- 139634967804952 -->\n<g class=\"node\" id=\"node8\">\n<title>139634967804952</title>\n<polygon fill=\"none\" points=\"130,-332.5 130,-378.5 382,-378.5 382,-332.5 130,-332.5\" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"183.5\" y=\"-351.8\">dense_3: Dense</text>\n<polyline fill=\"none\" points=\"237,-332.5 237,-378.5 \" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"266\" y=\"-363.3\">input:</text>\n<polyline fill=\"none\" points=\"237,-355.5 295,-355.5 \" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"266\" y=\"-340.3\">output:</text>\n<polyline fill=\"none\" points=\"295,-332.5 295,-378.5 \" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"338.5\" y=\"-363.3\">(None, 100)</text>\n<polyline fill=\"none\" points=\"295,-355.5 382,-355.5 \" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"338.5\" y=\"-340.3\">(None, 200)</text>\n</g>\n<!-- 139634967820608&#45;&gt;139634967804952 -->\n<g class=\"edge\" id=\"edge7\">\n<title>139634967820608-&gt;139634967804952</title>\n<path d=\"M256,-415.3799C256,-407.1745 256,-397.7679 256,-388.8786\" fill=\"none\" stroke=\"#000000\"/>\n<polygon fill=\"#000000\" points=\"259.5001,-388.784 256,-378.784 252.5001,-388.784 259.5001,-388.784\" stroke=\"#000000\"/>\n</g>\n<!-- 139634967783744 -->\n<g class=\"node\" id=\"node9\">\n<title>139634967783744</title>\n<polygon fill=\"none\" points=\"116.5,-249.5 116.5,-295.5 395.5,-295.5 395.5,-249.5 116.5,-249.5\" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"183.5\" y=\"-268.8\">dropout_4: Dropout</text>\n<polyline fill=\"none\" points=\"250.5,-249.5 250.5,-295.5 \" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"279.5\" y=\"-280.3\">input:</text>\n<polyline fill=\"none\" points=\"250.5,-272.5 308.5,-272.5 \" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"279.5\" y=\"-257.3\">output:</text>\n<polyline fill=\"none\" points=\"308.5,-249.5 308.5,-295.5 \" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"352\" y=\"-280.3\">(None, 200)</text>\n<polyline fill=\"none\" points=\"308.5,-272.5 395.5,-272.5 \" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"352\" y=\"-257.3\">(None, 200)</text>\n</g>\n<!-- 139634967804952&#45;&gt;139634967783744 -->\n<g class=\"edge\" id=\"edge8\">\n<title>139634967804952-&gt;139634967783744</title>\n<path d=\"M256,-332.3799C256,-324.1745 256,-314.7679 256,-305.8786\" fill=\"none\" stroke=\"#000000\"/>\n<polygon fill=\"#000000\" points=\"259.5001,-305.784 256,-295.784 252.5001,-305.784 259.5001,-305.784\" stroke=\"#000000\"/>\n</g>\n<!-- 139634967732912 -->\n<g class=\"node\" id=\"node10\">\n<title>139634967732912</title>\n<polygon fill=\"none\" points=\"130,-166.5 130,-212.5 382,-212.5 382,-166.5 130,-166.5\" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"183.5\" y=\"-185.8\">dense_4: Dense</text>\n<polyline fill=\"none\" points=\"237,-166.5 237,-212.5 \" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"266\" y=\"-197.3\">input:</text>\n<polyline fill=\"none\" points=\"237,-189.5 295,-189.5 \" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"266\" y=\"-174.3\">output:</text>\n<polyline fill=\"none\" points=\"295,-166.5 295,-212.5 \" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"338.5\" y=\"-197.3\">(None, 200)</text>\n<polyline fill=\"none\" points=\"295,-189.5 382,-189.5 \" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"338.5\" y=\"-174.3\">(None, 200)</text>\n</g>\n<!-- 139634967783744&#45;&gt;139634967732912 -->\n<g class=\"edge\" id=\"edge9\">\n<title>139634967783744-&gt;139634967732912</title>\n<path d=\"M256,-249.3799C256,-241.1745 256,-231.7679 256,-222.8786\" fill=\"none\" stroke=\"#000000\"/>\n<polygon fill=\"#000000\" points=\"259.5001,-222.784 256,-212.784 252.5001,-222.784 259.5001,-222.784\" stroke=\"#000000\"/>\n</g>\n<!-- 139634967691840 -->\n<g class=\"node\" id=\"node11\">\n<title>139634967691840</title>\n<polygon fill=\"none\" points=\"116.5,-83.5 116.5,-129.5 395.5,-129.5 395.5,-83.5 116.5,-83.5\" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"183.5\" y=\"-102.8\">dropout_5: Dropout</text>\n<polyline fill=\"none\" points=\"250.5,-83.5 250.5,-129.5 \" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"279.5\" y=\"-114.3\">input:</text>\n<polyline fill=\"none\" points=\"250.5,-106.5 308.5,-106.5 \" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"279.5\" y=\"-91.3\">output:</text>\n<polyline fill=\"none\" points=\"308.5,-83.5 308.5,-129.5 \" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"352\" y=\"-114.3\">(None, 200)</text>\n<polyline fill=\"none\" points=\"308.5,-106.5 395.5,-106.5 \" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"352\" y=\"-91.3\">(None, 200)</text>\n</g>\n<!-- 139634967732912&#45;&gt;139634967691840 -->\n<g class=\"edge\" id=\"edge10\">\n<title>139634967732912-&gt;139634967691840</title>\n<path d=\"M256,-166.3799C256,-158.1745 256,-148.7679 256,-139.8786\" fill=\"none\" stroke=\"#000000\"/>\n<polygon fill=\"#000000\" points=\"259.5001,-139.784 256,-129.784 252.5001,-139.784 259.5001,-139.784\" stroke=\"#000000\"/>\n</g>\n<!-- 139634967634720 -->\n<g class=\"node\" id=\"node12\">\n<title>139634967634720</title>\n<polygon fill=\"none\" points=\"130,-.5 130,-46.5 382,-46.5 382,-.5 130,-.5\" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"183.5\" y=\"-19.8\">dense_5: Dense</text>\n<polyline fill=\"none\" points=\"237,-.5 237,-46.5 \" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"266\" y=\"-31.3\">input:</text>\n<polyline fill=\"none\" points=\"237,-23.5 295,-23.5 \" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"266\" y=\"-8.3\">output:</text>\n<polyline fill=\"none\" points=\"295,-.5 295,-46.5 \" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"338.5\" y=\"-31.3\">(None, 200)</text>\n<polyline fill=\"none\" points=\"295,-23.5 382,-23.5 \" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"338.5\" y=\"-8.3\">(None, 1)</text>\n</g>\n<!-- 139634967691840&#45;&gt;139634967634720 -->\n<g class=\"edge\" id=\"edge11\">\n<title>139634967691840-&gt;139634967634720</title>\n<path d=\"M256,-83.3799C256,-75.1745 256,-65.7679 256,-56.8786\" fill=\"none\" stroke=\"#000000\"/>\n<polygon fill=\"#000000\" points=\"259.5001,-56.784 256,-46.784 252.5001,-56.784 259.5001,-56.784\" stroke=\"#000000\"/>\n</g>\n</g>\n</svg>"
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GUJYfKxKYVn1",
        "colab_type": "text"
      },
      "source": [
        "### Learning"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TzDdmrGrYVn5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "pat = 5 #ini adalah jumlah epoch tanpa peningkatan setelah pelatihan akan berhenti\n",
        "early_stopping = EarlyStopping(monitor='val_loss', patience=pat, verbose=1)\n",
        "\n",
        "#tentukan callback model pos pemeriksaan -> ini akan terus menyimpan model sebagai file fisik\n",
        "model_checkpoint = ModelCheckpoint('fas_mnist_1.h5', verbose=1, save_best_only=True)\n",
        "\n",
        "def fit_and_evaluate(t_x, val_x, t_y, val_y, EPOCHS=20, BATCH_SIZE=128):\n",
        "    results = model.fit(t_x, t_y, epochs=EPOCHS, batch_size=BATCH_SIZE, callbacks=[early_stopping, model_checkpoint], \n",
        "              verbose=1, validation_split=0.1)\n",
        "    print(\"Val Score: \", model.evaluate(val_x, val_y))\n",
        "    return results"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nNiCQ8_IYVn_",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "56f2e732-631e-4550-c228-c5393f63c8a9"
      },
      "source": [
        "n_folds=10\n",
        "epochs=5\n",
        "batch_size=128\n",
        "\n",
        "#simpan histori model dalam daftar setelah pemasangan sehingga kita dapat merencanakannya nanti\n",
        "model_history = [] \n",
        "\n",
        "for i in range(n_folds):\n",
        "    print(\"Training on Fold: \",i+1)\n",
        "    # train = tokenizeX(reviews_train_clean)\n",
        "    t_x, val_x, t_y, val_y = train_test_split(x_train, y_train, test_size=0.1, \n",
        "                                               random_state = np.random.randint(1,1000, 1)[0])\n",
        "    model_history.append(fit_and_evaluate(t_x, val_x, t_y, val_y, epochs, batch_size))\n",
        "    print(\"=======\"*12, end=\"\\n\\n\\n\")"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Training on Fold:  1\n",
            "Train on 20250 samples, validate on 2250 samples\n",
            "Epoch 1/5\n",
            "20250/20250 [==============================] - 2s 80us/step - loss: 0.7129 - acc: 0.4955 - val_loss: 0.6927 - val_acc: 0.4911\n",
            "\n",
            "Epoch 00001: val_loss improved from inf to 0.69268, saving model to fas_mnist_1.h5\n",
            "Epoch 2/5\n",
            "20250/20250 [==============================] - 1s 56us/step - loss: 0.6020 - acc: 0.6520 - val_loss: 0.4257 - val_acc: 0.8293\n",
            "\n",
            "Epoch 00002: val_loss improved from 0.69268 to 0.42569, saving model to fas_mnist_1.h5\n",
            "Epoch 3/5\n",
            "20250/20250 [==============================] - 1s 55us/step - loss: 0.4155 - acc: 0.8117 - val_loss: 0.3843 - val_acc: 0.8284\n",
            "\n",
            "Epoch 00003: val_loss improved from 0.42569 to 0.38435, saving model to fas_mnist_1.h5\n",
            "Epoch 4/5\n",
            "20250/20250 [==============================] - 1s 56us/step - loss: 0.3454 - acc: 0.8527 - val_loss: 0.4973 - val_acc: 0.7836\n",
            "\n",
            "Epoch 00004: val_loss did not improve from 0.38435\n",
            "Epoch 5/5\n",
            "20250/20250 [==============================] - 1s 55us/step - loss: 0.3046 - acc: 0.8733 - val_loss: 0.3061 - val_acc: 0.8778\n",
            "\n",
            "Epoch 00005: val_loss improved from 0.38435 to 0.30612, saving model to fas_mnist_1.h5\n",
            "2500/2500 [==============================] - 0s 52us/step\n",
            "Val Score:  [0.35249738426208493, 0.8624]\n",
            "====================================================================================\n",
            "\n",
            "\n",
            "Training on Fold:  2\n",
            "Train on 20250 samples, validate on 2250 samples\n",
            "Epoch 1/5\n",
            "20250/20250 [==============================] - 1s 57us/step - loss: 0.2840 - acc: 0.8847 - val_loss: 0.3105 - val_acc: 0.8756\n",
            "\n",
            "Epoch 00001: val_loss did not improve from 0.30612\n",
            "Epoch 2/5\n",
            "20250/20250 [==============================] - 1s 55us/step - loss: 0.2543 - acc: 0.8989 - val_loss: 0.2504 - val_acc: 0.9040\n",
            "\n",
            "Epoch 00002: val_loss improved from 0.30612 to 0.25041, saving model to fas_mnist_1.h5\n",
            "Epoch 3/5\n",
            "20250/20250 [==============================] - 1s 55us/step - loss: 0.2343 - acc: 0.9103 - val_loss: 0.2742 - val_acc: 0.8938\n",
            "\n",
            "Epoch 00003: val_loss did not improve from 0.25041\n",
            "Epoch 4/5\n",
            "20250/20250 [==============================] - 1s 55us/step - loss: 0.2152 - acc: 0.9162 - val_loss: 0.2826 - val_acc: 0.8893\n",
            "\n",
            "Epoch 00004: val_loss did not improve from 0.25041\n",
            "Epoch 5/5\n",
            "20250/20250 [==============================] - 1s 56us/step - loss: 0.2015 - acc: 0.9250 - val_loss: 0.3219 - val_acc: 0.8729\n",
            "\n",
            "Epoch 00005: val_loss did not improve from 0.25041\n",
            "2500/2500 [==============================] - 0s 47us/step\n",
            "Val Score:  [0.3238921712458134, 0.874]\n",
            "====================================================================================\n",
            "\n",
            "\n",
            "Training on Fold:  3\n",
            "Train on 20250 samples, validate on 2250 samples\n",
            "Epoch 1/5\n",
            "20250/20250 [==============================] - 1s 55us/step - loss: 0.2055 - acc: 0.9226 - val_loss: 0.2221 - val_acc: 0.9129\n",
            "\n",
            "Epoch 00001: val_loss improved from 0.25041 to 0.22211, saving model to fas_mnist_1.h5\n",
            "Epoch 2/5\n",
            "20250/20250 [==============================] - 1s 55us/step - loss: 0.1897 - acc: 0.9299 - val_loss: 0.2022 - val_acc: 0.9244\n",
            "\n",
            "Epoch 00002: val_loss improved from 0.22211 to 0.20219, saving model to fas_mnist_1.h5\n",
            "Epoch 3/5\n",
            "20250/20250 [==============================] - 1s 57us/step - loss: 0.1780 - acc: 0.9353 - val_loss: 0.2226 - val_acc: 0.9222\n",
            "\n",
            "Epoch 00003: val_loss did not improve from 0.20219\n",
            "Epoch 4/5\n",
            "20250/20250 [==============================] - 1s 55us/step - loss: 0.1671 - acc: 0.9403 - val_loss: 0.2298 - val_acc: 0.9178\n",
            "\n",
            "Epoch 00004: val_loss did not improve from 0.20219\n",
            "Epoch 5/5\n",
            "20250/20250 [==============================] - 1s 55us/step - loss: 0.1584 - acc: 0.9442 - val_loss: 0.2379 - val_acc: 0.9178\n",
            "\n",
            "Epoch 00005: val_loss did not improve from 0.20219\n",
            "2500/2500 [==============================] - 0s 45us/step\n",
            "Val Score:  [0.24142934592962265, 0.9152]\n",
            "====================================================================================\n",
            "\n",
            "\n",
            "Training on Fold:  4\n",
            "Train on 20250 samples, validate on 2250 samples\n",
            "Epoch 1/5\n",
            "20250/20250 [==============================] - 1s 55us/step - loss: 0.1678 - acc: 0.9406 - val_loss: 0.2579 - val_acc: 0.9058\n",
            "\n",
            "Epoch 00001: val_loss did not improve from 0.20219\n",
            "Epoch 2/5\n",
            "20250/20250 [==============================] - 1s 55us/step - loss: 0.1568 - acc: 0.9454 - val_loss: 0.2098 - val_acc: 0.9191\n",
            "\n",
            "Epoch 00002: val_loss did not improve from 0.20219\n",
            "Epoch 3/5\n",
            "20250/20250 [==============================] - 1s 55us/step - loss: 0.1447 - acc: 0.9507 - val_loss: 0.2075 - val_acc: 0.9253\n",
            "\n",
            "Epoch 00003: val_loss did not improve from 0.20219\n",
            "Epoch 4/5\n",
            "20250/20250 [==============================] - 1s 56us/step - loss: 0.1407 - acc: 0.9513 - val_loss: 0.2408 - val_acc: 0.9160\n",
            "\n",
            "Epoch 00004: val_loss did not improve from 0.20219\n",
            "Epoch 5/5\n",
            "20250/20250 [==============================] - 1s 55us/step - loss: 0.1347 - acc: 0.9552 - val_loss: 0.2264 - val_acc: 0.9187\n",
            "\n",
            "Epoch 00005: val_loss did not improve from 0.20219\n",
            "2500/2500 [==============================] - 0s 49us/step\n",
            "Val Score:  [0.21356189996302127, 0.9252]\n",
            "====================================================================================\n",
            "\n",
            "\n",
            "Training on Fold:  5\n",
            "Train on 20250 samples, validate on 2250 samples\n",
            "Epoch 1/5\n",
            "20250/20250 [==============================] - 1s 56us/step - loss: 0.1481 - acc: 0.9507 - val_loss: 0.1820 - val_acc: 0.9249\n",
            "\n",
            "Epoch 00001: val_loss improved from 0.20219 to 0.18204, saving model to fas_mnist_1.h5\n",
            "Epoch 2/5\n",
            "20250/20250 [==============================] - 1s 56us/step - loss: 0.1415 - acc: 0.9524 - val_loss: 0.1423 - val_acc: 0.9449\n",
            "\n",
            "Epoch 00002: val_loss improved from 0.18204 to 0.14233, saving model to fas_mnist_1.h5\n",
            "Epoch 3/5\n",
            "20250/20250 [==============================] - 1s 55us/step - loss: 0.1334 - acc: 0.9553 - val_loss: 0.1824 - val_acc: 0.9413\n",
            "\n",
            "Epoch 00003: val_loss did not improve from 0.14233\n",
            "Epoch 4/5\n",
            "20250/20250 [==============================] - 1s 55us/step - loss: 0.1262 - acc: 0.9585 - val_loss: 0.1912 - val_acc: 0.9347\n",
            "\n",
            "Epoch 00004: val_loss did not improve from 0.14233\n",
            "Epoch 5/5\n",
            "20250/20250 [==============================] - 1s 56us/step - loss: 0.1232 - acc: 0.9598 - val_loss: 0.1839 - val_acc: 0.9347\n",
            "\n",
            "Epoch 00005: val_loss did not improve from 0.14233\n",
            "2500/2500 [==============================] - 0s 44us/step\n",
            "Val Score:  [0.1984767236471176, 0.9256]\n",
            "====================================================================================\n",
            "\n",
            "\n",
            "Training on Fold:  6\n",
            "Train on 20250 samples, validate on 2250 samples\n",
            "Epoch 1/5\n",
            "20250/20250 [==============================] - 1s 55us/step - loss: 0.1349 - acc: 0.9540 - val_loss: 0.1117 - val_acc: 0.9653\n",
            "\n",
            "Epoch 00001: val_loss improved from 0.14233 to 0.11169, saving model to fas_mnist_1.h5\n",
            "Epoch 2/5\n",
            "20250/20250 [==============================] - 1s 55us/step - loss: 0.1289 - acc: 0.9571 - val_loss: 0.1427 - val_acc: 0.9498\n",
            "\n",
            "Epoch 00002: val_loss did not improve from 0.11169\n",
            "Epoch 3/5\n",
            "20250/20250 [==============================] - 1s 56us/step - loss: 0.1242 - acc: 0.9590 - val_loss: 0.1308 - val_acc: 0.9533\n",
            "\n",
            "Epoch 00003: val_loss did not improve from 0.11169\n",
            "Epoch 4/5\n",
            "20250/20250 [==============================] - 1s 55us/step - loss: 0.1176 - acc: 0.9615 - val_loss: 0.1413 - val_acc: 0.9498\n",
            "\n",
            "Epoch 00004: val_loss did not improve from 0.11169\n",
            "Epoch 5/5\n",
            "20250/20250 [==============================] - 1s 55us/step - loss: 0.1102 - acc: 0.9658 - val_loss: 0.3298 - val_acc: 0.8898\n",
            "\n",
            "Epoch 00005: val_loss did not improve from 0.11169\n",
            "2500/2500 [==============================] - 0s 46us/step\n",
            "Val Score:  [0.3058644416630268, 0.8996]\n",
            "====================================================================================\n",
            "\n",
            "\n",
            "Training on Fold:  7\n",
            "Train on 20250 samples, validate on 2250 samples\n",
            "Epoch 1/5\n",
            "20250/20250 [==============================] - 1s 56us/step - loss: 0.1215 - acc: 0.9610 - val_loss: 0.1095 - val_acc: 0.9636\n",
            "\n",
            "Epoch 00001: val_loss improved from 0.11169 to 0.10948, saving model to fas_mnist_1.h5\n",
            "Epoch 2/5\n",
            "20250/20250 [==============================] - 1s 57us/step - loss: 0.1111 - acc: 0.9639 - val_loss: 0.1518 - val_acc: 0.9440\n",
            "\n",
            "Epoch 00002: val_loss did not improve from 0.10948\n",
            "Epoch 3/5\n",
            "20250/20250 [==============================] - 1s 55us/step - loss: 0.1054 - acc: 0.9669 - val_loss: 0.2163 - val_acc: 0.9196\n",
            "\n",
            "Epoch 00003: val_loss did not improve from 0.10948\n",
            "Epoch 4/5\n",
            "20250/20250 [==============================] - 1s 55us/step - loss: 0.1019 - acc: 0.9684 - val_loss: 0.1469 - val_acc: 0.9467\n",
            "\n",
            "Epoch 00004: val_loss did not improve from 0.10948\n",
            "Epoch 5/5\n",
            "20250/20250 [==============================] - 1s 58us/step - loss: 0.0969 - acc: 0.9699 - val_loss: 0.2581 - val_acc: 0.9107\n",
            "\n",
            "Epoch 00005: val_loss did not improve from 0.10948\n",
            "2500/2500 [==============================] - 0s 52us/step\n",
            "Val Score:  [0.28715692648887636, 0.9028]\n",
            "====================================================================================\n",
            "\n",
            "\n",
            "Training on Fold:  8\n",
            "Train on 20250 samples, validate on 2250 samples\n",
            "Epoch 1/5\n",
            "20250/20250 [==============================] - 1s 55us/step - loss: 0.1130 - acc: 0.9638 - val_loss: 0.1262 - val_acc: 0.9538\n",
            "\n",
            "Epoch 00001: val_loss did not improve from 0.10948\n",
            "Epoch 2/5\n",
            "20250/20250 [==============================] - 1s 55us/step - loss: 0.1021 - acc: 0.9684 - val_loss: 0.1310 - val_acc: 0.9551\n",
            "\n",
            "Epoch 00002: val_loss did not improve from 0.10948\n",
            "Epoch 3/5\n",
            "20250/20250 [==============================] - 1s 55us/step - loss: 0.0980 - acc: 0.9697 - val_loss: 0.1322 - val_acc: 0.9533\n",
            "\n",
            "Epoch 00003: val_loss did not improve from 0.10948\n",
            "Epoch 4/5\n",
            "20250/20250 [==============================] - 1s 56us/step - loss: 0.0949 - acc: 0.9704 - val_loss: 0.1324 - val_acc: 0.9542\n",
            "\n",
            "Epoch 00004: val_loss did not improve from 0.10948\n",
            "Epoch 5/5\n",
            "20250/20250 [==============================] - 1s 56us/step - loss: 0.0886 - acc: 0.9737 - val_loss: 0.1545 - val_acc: 0.9462\n",
            "\n",
            "Epoch 00005: val_loss did not improve from 0.10948\n",
            "2500/2500 [==============================] - 0s 47us/step\n",
            "Val Score:  [0.14986065986752511, 0.9476]\n",
            "====================================================================================\n",
            "\n",
            "\n",
            "Training on Fold:  9\n",
            "Train on 20250 samples, validate on 2250 samples\n",
            "Epoch 1/5\n",
            "20250/20250 [==============================] - 1s 55us/step - loss: 0.0983 - acc: 0.9698 - val_loss: 0.0914 - val_acc: 0.9720\n",
            "\n",
            "Epoch 00001: val_loss improved from 0.10948 to 0.09136, saving model to fas_mnist_1.h5\n",
            "Epoch 2/5\n",
            "20250/20250 [==============================] - 1s 55us/step - loss: 0.0907 - acc: 0.9716 - val_loss: 0.1300 - val_acc: 0.9547\n",
            "\n",
            "Epoch 00002: val_loss did not improve from 0.09136\n",
            "Epoch 3/5\n",
            "20250/20250 [==============================] - 1s 56us/step - loss: 0.0845 - acc: 0.9758 - val_loss: 0.1207 - val_acc: 0.9604\n",
            "\n",
            "Epoch 00003: val_loss did not improve from 0.09136\n",
            "Epoch 4/5\n",
            "20250/20250 [==============================] - 1s 55us/step - loss: 0.0811 - acc: 0.9756 - val_loss: 0.1174 - val_acc: 0.9604\n",
            "\n",
            "Epoch 00004: val_loss did not improve from 0.09136\n",
            "Epoch 5/5\n",
            "20250/20250 [==============================] - 1s 55us/step - loss: 0.0749 - acc: 0.9786 - val_loss: 0.1971 - val_acc: 0.9360\n",
            "\n",
            "Epoch 00005: val_loss did not improve from 0.09136\n",
            "2500/2500 [==============================] - 0s 44us/step\n",
            "Val Score:  [0.21347394853830337, 0.928]\n",
            "====================================================================================\n",
            "\n",
            "\n",
            "Training on Fold:  10\n",
            "Train on 20250 samples, validate on 2250 samples\n",
            "Epoch 1/5\n",
            "20250/20250 [==============================] - 1s 56us/step - loss: 0.0843 - acc: 0.9739 - val_loss: 0.0797 - val_acc: 0.9791\n",
            "\n",
            "Epoch 00001: val_loss improved from 0.09136 to 0.07971, saving model to fas_mnist_1.h5\n",
            "Epoch 2/5\n",
            "20250/20250 [==============================] - 1s 55us/step - loss: 0.0763 - acc: 0.9768 - val_loss: 0.1253 - val_acc: 0.9569\n",
            "\n",
            "Epoch 00002: val_loss did not improve from 0.07971\n",
            "Epoch 3/5\n",
            "20250/20250 [==============================] - 1s 55us/step - loss: 0.0688 - acc: 0.9800 - val_loss: 0.1367 - val_acc: 0.9556\n",
            "\n",
            "Epoch 00003: val_loss did not improve from 0.07971\n",
            "Epoch 4/5\n",
            "20250/20250 [==============================] - 1s 56us/step - loss: 0.0658 - acc: 0.9814 - val_loss: 0.3063 - val_acc: 0.9058\n",
            "\n",
            "Epoch 00004: val_loss did not improve from 0.07971\n",
            "Epoch 5/5\n",
            "20250/20250 [==============================] - 1s 55us/step - loss: 0.0618 - acc: 0.9821 - val_loss: 0.1293 - val_acc: 0.9627\n",
            "\n",
            "Epoch 00005: val_loss did not improve from 0.07971\n",
            "2500/2500 [==============================] - 0s 48us/step\n",
            "Val Score:  [0.12845237897187473, 0.9624]\n",
            "====================================================================================\n",
            "\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Nnku7fzHYVoB",
        "colab_type": "text"
      },
      "source": [
        "### Inference"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PFeWwIPSYVoC",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "9f39beeb-487d-41f9-f4f7-2174f5052b29"
      },
      "source": [
        "scores = model.evaluate(x_test, y_test, verbose=2)\n",
        "print(\"%s: %.2f%%\" % (model.metrics_names[1], scores[1]*100))"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "acc: 82.24%\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QsVbUnQnYVoE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}